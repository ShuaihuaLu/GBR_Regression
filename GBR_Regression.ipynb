{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3e238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin'\n",
    "\n",
    "\n",
    "import xgboost\n",
    "from hyperopt import fmin, tpe, hp, rand, anneal, partial, Trials\n",
    "\n",
    "import  xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "np.set_printoptions(suppress=True)\n",
    "xlrd.xlsx.ensure_elementtree_imported(False, None)\n",
    "xlrd.xlsx.Element_has_iter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74229a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Build Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_load(filename, Sheet):\n",
    "    data = pd.read_excel(filename, sheet_name=Sheet)  # Read the specified sheet from the excel file\n",
    "    result = data.values.tolist()  # Convert the data to a nested list\n",
    "    nrows, ncols = data.shape  # Get the number of rows and columns\n",
    "\n",
    "    ID = [int(x[0]) for x in result]  # Get the ID list from the first column\n",
    "    formula = [x[1] for x in result]  # Get the chemical formula list from the second column\n",
    "    prototype = [x[2] for x in result]  # Get the initial structure name list from the third column\n",
    "    features = np.array([x[3:ncols-1] for x in result])  # Get the feature matrix from the fourth column onwards\n",
    "    label = np.array([x[-1] for x in result])  # Get the label column from the last column\n",
    "    features_name = np.array(data.columns[3:ncols-1])  # Get the feature names from the header row\n",
    "\n",
    "    return ID, formula, prototype, features, label, features_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e85530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Handle missing values in the dataset using mean imputation\n",
    "def imputer(dataArr):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp.fit(dataArr)\n",
    "    dataArr_full = imp.transform(dataArr)\n",
    "    return dataArr_full\n",
    "\n",
    "# Normalize/standardize the features in X and the target variable Y\n",
    "def normData(dataX, dataY):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(dataX)\n",
    "    Y = scaler.fit_transform(dataY)\n",
    "    return X, Y\n",
    "\n",
    "# Split the dataset into training and testing sets using the hold-out method\n",
    "def splitDataHO(X, Y, testSize, random_state):\n",
    "    trainX, testX, trainY, testY = train_test_split(X, Y, test_size=testSize, random_state=random_state)\n",
    "    return trainX, testX, trainY, testY\n",
    "\n",
    "# Split the dataset into training/testing data and prediction data\n",
    "def split_train_predict(X, Y, number_train_test):\n",
    "    items_number = len(X)\n",
    "    train_test_data_Arr = []\n",
    "    predict_data_Arr = []\n",
    "    train_test_label_Arr = []\n",
    "    predict_label_Arr = []\n",
    "\n",
    "    for i in range(number_train_test):\n",
    "        train_test_data = X[i]\n",
    "        train_test_label = Y[i]\n",
    "        train_test_data_Arr.append(train_test_data)\n",
    "        train_test_label_Arr.append(train_test_label)\n",
    "\n",
    "    for j in range(number_train_test, items_number):\n",
    "        predict_data = X[j]\n",
    "        predict_label = Y[j]\n",
    "        predict_data_Arr.append(predict_data)\n",
    "        predict_label_Arr.append(predict_label)\n",
    "\n",
    "    return np.array(train_test_data_Arr), np.array(train_test_label_Arr), np.array(predict_data_Arr), np.array(predict_label_Arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f5fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Model Evaluation\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "\n",
    "def ModelEvaluationRegression(testY, prediction):\n",
    "    # Calculate R2 score\n",
    "    R2 = r2_score(testY, prediction)\n",
    "    # Calculate mean squared error\n",
    "    MSE = mean_squared_error(testY, prediction)\n",
    "    # Calculate mean absolute error\n",
    "    MAE = mean_absolute_error(testY, prediction)\n",
    "    # Calculate explained variance\n",
    "    EV = explained_variance_score(testY, prediction)\n",
    "\n",
    "    return R2, MSE, MAE, EV\n",
    "\n",
    "def hyper_parameters_plot(parameters, trials, Loop_Step, screen_step):\n",
    "    matplotlib_axes_logger.setLevel('ERROR')\n",
    "    \n",
    "    # Create subplots for parameter visualization\n",
    "    f, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10), dpi=330)\n",
    "    cmap = plt.cm.jet\n",
    "    \n",
    "    for i, val in enumerate(parameters):\n",
    "        # Extract parameter values and corresponding losses\n",
    "        xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "        ys = [-t['result']['loss'] for t in trials.trials]\n",
    "        xs, ys = zip(*sorted(zip(xs, ys)))\n",
    "\n",
    "        # Scatter plot of parameter values and losses\n",
    "        axes[int(i / 3), int(i % 3)].scatter(xs, ys, s=50, linewidth=0.01, alpha=0.5, norm=0.5,\n",
    "                                             c=cmap(float(i) / len(parameters)))\n",
    "        axes[int(i / 3), int(i % 3)].set_title(val, fontsize=20, fontweight=\"bold\", fontname='Arial')\n",
    "\n",
    "        # Set tick parameters and axis limits\n",
    "        axes[int(i / 3), int(i % 3)].tick_params(labelsize=18, direction='out', width=2, length=6)\n",
    "        axes[int(i / 3), int(i % 3)].set_ylim((0.5, 1))\n",
    "        labels = axes[int(i / 3), int(i % 3)].get_xticklabels() + axes[int(i / 3), int(i % 3)].get_yticklabels()\n",
    "        [label.set_fontname('Arial') for label in labels]\n",
    "        [label.set_fontweight('bold') for label in labels]\n",
    "        axes[int(i / 3), int(i % 3)].spines['top'].set_linewidth(2.5)\n",
    "        axes[int(i / 3), int(i % 3)].spines['bottom'].set_linewidth(2.5)\n",
    "        axes[int(i / 3), int(i % 3)].spines['right'].set_linewidth(2.5)\n",
    "        axes[int(i / 3), int(i % 3)].spines['left'].set_linewidth(2.5)\n",
    "    \n",
    "    # Adjust subplot layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_path + 'hyper_parameters_trials_' + str(Loop_Step) + \"_\" + str(screen_step) + '.png', format='png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_regression_results(trainY, testY, pred_trainY, std_pred_trainY, pred_testY, std_pred_testY, Loop_Step, screen_step):\n",
    "    # Calculate R2 score and mean absolute error for training set\n",
    "    R2_average_train = r2_score(trainY, pred_trainY)\n",
    "    MAE_average_train = mean_absolute_error(trainY, pred_trainY)\n",
    "    # Calculate R2 score and mean absolute error for test set\n",
    "    R2_average_test = r2_score(testY, pred_testY)\n",
    "    MAE_average_test = mean_absolute_error(testY, pred_testY)\n",
    "\n",
    "    tolerance = 0.1\n",
    "    plt.figure(figsize=(10, 10), dpi=330)\n",
    "    \n",
    "    # Plot the ideal line\n",
    "    plt.plot([testY.min()-tolerance, testY.max()+tolerance],\n",
    "             [testY.min()-tolerance, testY.max()+tolerance],\n",
    "             '--r', linewidth=3)\n",
    "    \n",
    "    # Plot the training set results with error bars\n",
    "    plt.errorbar(trainY, pred_trainY, yerr=std_pred_trainY, alpha=0.75, fmt='o', ms=13, mfc=(0 / 255, 98 / 255, 132 / 255),\n",
    "                 ecolor=(55 / 255, 60 / 255, 56 / 255), elinewidth=3, capsize=5, capthick=3,\n",
    "                 label=r'train $R^2=$ %.2f  $MAE=$ %.2f' % (R2_average_train, MAE_average_train))\n",
    "    \n",
    "    # Plot the test set results with error bars\n",
    "    plt.errorbar(testY, pred_testY, yerr=std_pred_testY, alpha=0.75, fmt='o', ms=13, mfc=(232 / 255, 48 / 255, 21 / 255),\n",
    "                 ecolor=(55 / 255, 60 / 255, 56 / 255), elinewidth=3, capsize=5, capthick=3,\n",
    "                 label=r'test $R^2=$ %.2f  $MAE=$ %.2f' % (R2_average_test, MAE_average_test))\n",
    "    \n",
    "    # Set x and y limits\n",
    "    plt.xlim([testY.min()-tolerance, testY.max()+tolerance])\n",
    "    plt.ylim([testY.min()-tolerance, testY.max()+tolerance])\n",
    "    \n",
    "    # Set x and y labels, legend, and tick parameters\n",
    "    plt.xlabel('DFT Calculated ${E_{FM-AFM}}$ (eV)', fontsize=26, fontweight=\"bold\", fontname='Arial')\n",
    "    plt.ylabel('ML Predicted ${E_{FM-AFM}}$ (eV)', fontsize=26, fontweight=\"bold\", fontname='Arial')\n",
    "    plt.legend(loc='upper left')\n",
    "    leg = plt.gca().get_legend()\n",
    "    ltext = leg.get_texts()\n",
    "    bwith = 3\n",
    "    ax = plt.gca()\n",
    "    ax.spines['bottom'].set_linewidth(bwith)\n",
    "    ax.spines['left'].set_linewidth(bwith)\n",
    "    ax.spines['top'].set_linewidth(bwith)\n",
    "    ax.spines['right'].set_linewidth(bwith)\n",
    "    plt.setp(ltext, fontsize=24, fontweight='bold', fontname='Arial')\n",
    "    plt.tick_params(direction='out', width=2, length=6)\n",
    "    labels = ax.get_xticklabels() + ax.get_yticklabels()\n",
    "    [label.set_fontname('Arial') for label in labels]\n",
    "    [label.set_fontweight('bold') for label in labels]\n",
    "    [label.set_fontsize(24) for label in labels]\n",
    "    plt.tight_layout()\n",
    "    plt.title('DFT Calculated Values vs ML Predicted Values', fontsize=26, fontweight=\"bold\", fontname='Arial')\n",
    "    \n",
    "    # Save the figure and close the plot\n",
    "    plt.savefig(result_path + \"FM_Regression_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".png\", format=\"png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8445b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Parameter Selection\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def GBRegression(trainX, testX, trainY, testY, max_evals, Loop_Step, screen_step):\n",
    "    # Open files for storing results\n",
    "    GBR_model_score_filename = open(result_path + \"hyper_parameter_selection_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    predicted_testY_file = open(result_path + \"predicted_testY_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".dat\", 'a')\n",
    "    predicted_trainY_file = open(result_path + \"predicted_trainY_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".dat\", 'a')\n",
    "\n",
    "    # Define the parameter space for hyperopt\n",
    "    parameter_space_gbr = {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "        \"max_depth\": hp.quniform(\"max_depth\", 1, 10, 1),\n",
    "        \"n_estimators\": hp.quniform(\"n_estimators\", 10, 200, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.5, 10),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.01, 1)\n",
    "    }\n",
    "\n",
    "    count = 0  # Counter for parameter combinations\n",
    "\n",
    "    def function(argsDict):\n",
    "        # Extract parameter values from argsDict\n",
    "        colsample_bytree = argsDict[\"colsample_bytree\"]\n",
    "        max_depth = argsDict[\"max_depth\"]\n",
    "        n_estimators = argsDict['n_estimators']\n",
    "        learning_rate = argsDict[\"learning_rate\"]\n",
    "        subsample = argsDict[\"subsample\"]\n",
    "        min_child_weight = argsDict[\"min_child_weight\"]\n",
    "        gamma = argsDict[\"gamma\"]\n",
    "\n",
    "        # Create and train the XGBoost regression model\n",
    "        clf = xgb.XGBRegressor(nthread=4,\n",
    "                               colsample_bytree=colsample_bytree,\n",
    "                               max_depth=int(max_depth),\n",
    "                               n_estimators=int(n_estimators),\n",
    "                               learning_rate=learning_rate,\n",
    "                               subsample=subsample,\n",
    "                               min_child_weight=min_child_weight,\n",
    "                               gamma=gamma,\n",
    "                               random_state=int(42),\n",
    "                               objective=\"reg:squarederror\"\n",
    "                               )\n",
    "        clf.fit(trainX, trainY)\n",
    "        prediction = clf.predict(testX)\n",
    "\n",
    "        nonlocal count\n",
    "        count += 1\n",
    "\n",
    "        # Evaluate the model's performance\n",
    "        R2, MSE, MAE, EV = ModelEvaluationRegression(testY, prediction)\n",
    "        print(\"No.%s, R2: %f, MSE: %f, MAE: %f, EV: %f\" % (str(count), R2, MSE, MAE, EV), argsDict, file=GBR_model_score_filename)\n",
    "        print(\"Screen_step: %s, Loop: %s, No. %s, R2: %f, MSE: %f, MAE: %f, EV: %f\" %\n",
    "              (str(screen_step), str(Loop_Step), str(count), R2, MSE, MAE, EV))\n",
    "\n",
    "        # Return the negative R2 value for hyperopt to maximize\n",
    "        return -R2\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(function, parameter_space_gbr, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    parameters = ['colsample_bytree', 'max_depth', 'n_estimators', 'learning_rate', 'gamma', 'min_child_weight']\n",
    "    hyper_parameters_plot(parameters, trials, Loop_Step, screen_step)\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    colsample_bytree = best[\"colsample_bytree\"]\n",
    "    max_depth = best[\"max_depth\"]\n",
    "    n_estimators = best['n_estimators']\n",
    "    learning_rate = best[\"learning_rate\"]\n",
    "    subsample = best[\"subsample\"]\n",
    "    min_child_weight = best[\"min_child_weight\"]\n",
    "    gamma = best[\"gamma\"]\n",
    "\n",
    "    print(\"The_best_parameter：\", best, file=GBR_model_score_filename)\n",
    "\n",
    "    # Train the best model using the best hyperparameters\n",
    "    best_model = xgb.XGBRegressor(nthread=4,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  max_depth=int(max_depth),\n",
    "                                  n_estimators=int(n_estimators),\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  subsample=subsample,\n",
    "                                  min_child_weight=min_child_weight,\n",
    "                                  gamma=gamma,\n",
    "                                  random_state=int(42),\n",
    "                                  objective=\"reg:squarederror\"\n",
    "                                  )\n",
    "    best_model.fit(trainX, trainY)\n",
    "    best_model_pred_testY = best_model.predict(testX)\n",
    "    best_model_pred_trainY = best_model.predict(trainX)\n",
    "    R2_test, MSE_test, MAE_test, EV_test = ModelEvaluationRegression(testY, best_model_pred_testY)\n",
    "    R2_train, MSE_train, MAE_train, EV_train = ModelEvaluationRegression(trainY, best_model_pred_trainY)\n",
    "    print(\"The_best_model_train_score:\", R2_train, MSE_train, MAE_train, EV_train, file=GBR_model_score_filename)\n",
    "    print(\"The_best_model_test_score:\", R2_test, MSE_test, MAE_test, EV_test, file=GBR_model_score_filename)\n",
    "\n",
    "    # Save the best model and visualization\n",
    "    best_model.save_model(result_path + \"best_model_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".model\")\n",
    "    digraph = xgboost.to_graphviz(best_model)\n",
    "    digraph_name = result_path + \"plot_best_tree_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".gv\"\n",
    "    digraph.save(filename=digraph_name)\n",
    "\n",
    "    # Write predicted values to files\n",
    "    for y in best_model_pred_testY:\n",
    "        print(y, file=predicted_testY_file)\n",
    "    for y in best_model_pred_trainY:\n",
    "        print(y, file=predicted_trainY_file)\n",
    "\n",
    "    # Close the files\n",
    "    GBR_model_score_filename.close()\n",
    "    predicted_testY_file.close()\n",
    "    predicted_trainY_file.close()\n",
    "\n",
    "    return best_model, best_model_pred_trainY, best_model_pred_testY, \\\n",
    "           R2_train, MSE_train, MAE_train, EV_train, \\\n",
    "           R2_test, MSE_test, MAE_test, EV_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea128c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Prediction\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Function to create directory if it doesn't exist\n",
    "def path_mkdir(path, result_subfile, screen_step):\n",
    "    feature_engineering_result_file = result_file + str(result_subfile) + str(screen_step)\n",
    "    if not os.path.exists(result_file):\n",
    "        os.mkdir(path + result_file)\n",
    "    if not os.path.exists(feature_engineering_result_file):\n",
    "        os.mkdir(path + feature_engineering_result_file)\n",
    "    result_path = path + feature_engineering_result_file + \"/\"\n",
    "    return result_path\n",
    "\n",
    "def train_test(trainX, testX, trainY, testY, screen_step):\n",
    "    pred_testArr = []              # List to store predicted test values\n",
    "    mean_pred_testArr = []         # List to store mean of predicted test values\n",
    "    std_pred_testArr = []          # List to store standard deviation of predicted test values\n",
    "    pred_trainArr = []             # List to store predicted train values\n",
    "    mean_pred_trainArr = []        # List to store mean of predicted train values\n",
    "    std_pred_trainArr = []         # List to store standard deviation of predicted train values\n",
    "    features_importanceArr = []    # List to store feature importances\n",
    "    mean_features_importanceArr = []   # List to store mean of feature importances\n",
    "    std_features_importanceArr = []    # List to store standard deviation of feature importances\n",
    "\n",
    "    for n in range(LoopStepMin, LoopStepMax):\n",
    "        # Perform GBRegression and store the results\n",
    "        best_model, best_model_pred_trainY, best_model_pred_testY, R2_train, MSE_train, MAE_train, EV_train, R2_test, MSE_test, MAE_test, EV_test = \\\n",
    "            GBRegression(trainX, testX, trainY, testY, HyperParameter_Step, n, screen_step)\n",
    "        pred_trainArr.append(best_model_pred_trainY)\n",
    "        pred_testArr.append(best_model_pred_testY)\n",
    "        features_importance = best_model.feature_importances_\n",
    "        features_importanceArr.append(features_importance)\n",
    "    \n",
    "    # Calculate mean and standard deviation of predicted test values\n",
    "    for i in range(np.shape(pred_testArr)[1]):\n",
    "        sample = np.array(pred_testArr)[:, i]\n",
    "        testsample_prediction_mean = np.mean(sample)\n",
    "        testsample_prediction_std = np.std(sample, ddof=0)\n",
    "        mean_pred_testArr.append(testsample_prediction_mean)\n",
    "        std_pred_testArr.append(testsample_prediction_std)\n",
    "    \n",
    "    # Calculate mean and standard deviation of predicted train values\n",
    "    for j in range(np.shape(pred_trainArr)[1]):\n",
    "        sample = np.array(pred_trainArr)[:, j]\n",
    "        trainsample_prediction_mean = np.mean(sample)\n",
    "        trainsample_prediction_std = np.std(sample, ddof=0)\n",
    "        mean_pred_trainArr.append(trainsample_prediction_mean)\n",
    "        std_pred_trainArr.append(trainsample_prediction_std)\n",
    "    \n",
    "    # Calculate mean and standard deviation of feature importances\n",
    "    for k in range(np.shape(features_importanceArr)[1]):\n",
    "        sample = np.array(features_importanceArr)[:, k]\n",
    "        features_importance_mean = np.mean(sample)\n",
    "        features_importance_std = np.std(sample, ddof=0)\n",
    "        mean_features_importanceArr.append(features_importance_mean)\n",
    "        std_features_importanceArr.append(features_importance_std)\n",
    "\n",
    "    # Plot regression results\n",
    "    plot_regression_results(trainY, testY, np.array(mean_pred_trainArr), np.array(std_pred_trainArr), np.array(mean_pred_testArr), \\\n",
    "                            np.array(std_pred_testArr), 'average', screen_step)\n",
    "\n",
    "    # Open files for writing predicted train, test, and feature importance values\n",
    "    train_font = open(result_path + \"predicted_train_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    test_font = open(result_path + \"predicted_test_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    features_importance_font = open(result_path + \"features_importance_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    \n",
    "    # Write mean and standard deviation of predicted train values to file\n",
    "    for i, j in zip(mean_pred_trainArr, std_pred_trainArr):\n",
    "        print(i, j, file=train_font)\n",
    "    \n",
    "    # Write mean and standard deviation of predicted test values to file\n",
    "    for i, j in zip(mean_pred_testArr, std_pred_testArr):\n",
    "        print(i, j, file=test_font)\n",
    "    \n",
    "    # Write feature names, mean, and standard deviation of feature importances to file\n",
    "    for i, j, k in zip(features_name, mean_features_importanceArr, std_features_importanceArr):\n",
    "        print(i, j, k, file=features_importance_font)\n",
    "    \n",
    "    # Close the files\n",
    "    train_font.close()\n",
    "    test_font.close()\n",
    "    features_importance_font.close()\n",
    "    \n",
    "    # Return the calculated values\n",
    "    return mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, mean_features_importanceArr, std_features_importanceArr\n",
    "\n",
    "def predict(trainX, trainY, testX, testY, predictX, screen_step):\n",
    "    print(\"Predicting......\")\n",
    "    predict_font = open(result_path + \"predicted_predict_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    train_set = pd.DataFrame(trainX, columns=features_name)\n",
    "    dtrain = xgboost.DMatrix(train_set)\n",
    "    test_set = pd.DataFrame(testX, columns=features_name)\n",
    "    dtest = xgboost.DMatrix(test_set)\n",
    "    predict_set = pd.DataFrame(predictX, columns=features_name)\n",
    "    dpredict = xgboost.DMatrix(predict_set)\n",
    "    pred_predict_Arr = []               # List to store predicted values for predictX\n",
    "    mean_pred_predict_Arr = []          # List to store mean of predicted values for predictX\n",
    "    std_pred_predict_Arr = []           # List to store standard deviation of predicted values for predictX\n",
    "    \n",
    "    for i in range(LoopStepMin, LoopStepMax):\n",
    "        model_file = result_path + \"best_model_\" + str(i) + \"_FF.model\"\n",
    "        best_model = xgboost.Booster(model_file=model_file)\n",
    "        best_model_pred_testY = best_model.predict(dtest)\n",
    "        best_model_pred_trainY = best_model.predict(dtrain)\n",
    "        best_model_pred_predictY = best_model.predict(dpredict)\n",
    "        R2_test, MSE_test, MAE_test, EV_test = ModelEvaluationRegression(testY, best_model_pred_testY)\n",
    "        R2_train, MSE_train, MAE_train, EV_train = ModelEvaluationRegression(trainY, best_model_pred_trainY)\n",
    "        print(\"R2_train: %f, MSE_train: %f, MAE_train: %f, EV_train: %f\" % (R2_train, MSE_train, MAE_train, EV_train))\n",
    "        print(\"R2_test: %f, MSE_test: %f, MAE_test: %f, EV_test: %f\" % (R2_test, MSE_test, MAE_test, EV_test))\n",
    "        pred_predict_Arr.append(best_model_pred_predictY)\n",
    "    \n",
    "    # Calculate mean and standard deviation of predicted values for predictX\n",
    "    for i in range(np.shape(pred_predict_Arr)[1]):\n",
    "        sample = np.array(pred_predict_Arr)[:, i]\n",
    "        predictsample_prediction_mean = np.mean(sample)\n",
    "        predictsample_prediction_std = np.std(sample, ddof=0)\n",
    "        mean_pred_predict_Arr.append(predictsample_prediction_mean)\n",
    "        std_pred_predict_Arr.append(predictsample_prediction_std)\n",
    "    \n",
    "    # Write mean and standard deviation of predicted values for predictX to file\n",
    "    for i, j in zip(mean_pred_predict_Arr, std_pred_predict_Arr):\n",
    "        print(i, j, file=predict_font)\n",
    "    \n",
    "    # Return the calculated values\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d56dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- feature engineering\n",
    "# -----------------------------------------------------------------------------\n",
    "def feature_importance(features_name, features, screen_step):\n",
    "    # Open files to write normalized and original dataset\n",
    "    normdata_filename = open(result_path + \"norm_dataset_\" + str(screen_step) +\".dat\", \"a\")\n",
    "    data_filename = open(result_path + \"dataset_\" + str(screen_step) +\".dat\", \"a\")\n",
    "    \n",
    "    if n_fixed_features != 0:\n",
    "        # Extract vector features\n",
    "        vector_features = features[:, n_fixed_features: np.shape(features)[1]]\n",
    "        vector_features_name = list(features_name[n_fixed_features: np.shape(features)[1]])\n",
    "        \n",
    "        # Concatenate fixed features with vector features\n",
    "        new_features = np.concatenate((fixed_features, vector_features), axis=1)\n",
    "        \n",
    "        # Normalize features and labels\n",
    "        norm_features, norm_label = normData(new_features, label)\n",
    "        \n",
    "        # Concatenate fixed feature names with vector feature names\n",
    "        new_features_name = np.concatenate((initial_fixed_features_name, vector_features_name), axis=0)\n",
    "        \n",
    "        # Write normalized dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in new_features_name], \"label\", file=normdata_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, norm_features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=normdata_filename)\n",
    "        \n",
    "        # Write original dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in new_features_name], \"label\", file=data_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=data_filename)\n",
    "    else:\n",
    "        # Use all features as vector features\n",
    "        vector_features = features\n",
    "        vector_features_name = list(features_name)\n",
    "        \n",
    "        # Normalize vector features and labels\n",
    "        norm_vector_features, norm_label = normData(vector_features, label)\n",
    "        norm_features = norm_vector_features\n",
    "        \n",
    "        # Write normalized dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in vector_features_name], \"label\", file=normdata_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, norm_features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=normdata_filename)\n",
    "        \n",
    "        # Write original dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in vector_features_name], \"label\", file=data_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, vector_features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=data_filename)\n",
    "\n",
    "    # Split data into train-test sets\n",
    "    train_test_data_Arr, train_test_label_Arr, predict_data_Arr, predict_label_Arr = split_train_predict(norm_features,\n",
    "                                                                                                         norm_label,\n",
    "                                                                                                         number_sample)\n",
    "    trainX, testX, trainY, testY = splitDataHO(train_test_data_Arr, train_test_label_Arr, TestSetRatio, RandomSeed)\n",
    "\n",
    "    # Train and test the model\n",
    "    mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "    mean_features_importanceArr, std_features_importanceArr = train_test(trainX, testX, trainY, testY, screen_step)\n",
    "\n",
    "    normdata_filename.close()\n",
    "    data_filename.close()\n",
    "    \n",
    "    # Return the results\n",
    "    return mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "           mean_features_importanceArr, std_features_importanceArr, \\\n",
    "           fixed_features_name, fixed_features, vector_features_name, vector_features\n",
    "\n",
    "\n",
    "def fixed_feature_engineering(mean_features_importanceArr, std_features_importanceArr, fixed_features_name,\n",
    "                              fixed_features, vector_features_name, vector_features, screen_step):\n",
    "    sorted_features_importance = []\n",
    "    sorted_features_importance_std = []\n",
    "    \n",
    "    shape_fixed_features = np.shape(fixed_features)\n",
    "    \n",
    "    # Calculate importance of fixed features\n",
    "    fixed_features_importance = np.float(np.sum(mean_features_importanceArr[0: shape_fixed_features[1]]))\n",
    "    fixed_features_importance_std = np.sum(std_features_importanceArr[0: shape_fixed_features[1]]) / shape_fixed_features[1]\n",
    "    \n",
    "    # Calculate importance of vector features\n",
    "    vector_features_importance = mean_features_importanceArr[shape_fixed_features[1]: len(mean_features_importanceArr)]\n",
    "    vector_features_importance_std = std_features_importanceArr[shape_fixed_features[1]: len(mean_features_importanceArr)]\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_idx = np.argsort(vector_features_importance)[::-1]\n",
    "    sorted_vector_features_name = np.array(vector_features_name)[sorted_idx]\n",
    "    sorted_vector_features_importance = np.array(vector_features_importance)[sorted_idx]\n",
    "    sorted_vector_features_importance_std = np.array(vector_features_importance_std)[sorted_idx]\n",
    "    \n",
    "    row_number = np.shape(vector_features)[0]\n",
    "    vfeature_column_number = np.shape(vector_features)[1]\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_vector_features = np.zeros((row_number, vfeature_column_number))\n",
    "    for i, j in zip(sorted_idx, np.arange(0, vfeature_column_number)):\n",
    "        sorted_vector_features[:, j] = vector_features[:, i]\n",
    "    \n",
    "    # Concatenate fixed features with sorted vector features\n",
    "    sorted_features = np.concatenate((fixed_features, sorted_vector_features), axis=1)\n",
    "    \n",
    "    # Remove last feature name and feature column from sorted vector features\n",
    "    selected_sorted_vector_features_name = np.delete(sorted_vector_features_name, -1)\n",
    "    selected_sorted_features = np.delete(sorted_features, -1, axis=1)\n",
    "    \n",
    "    # Concatenate fixed feature names with sorted vector feature names\n",
    "    sorted_features_name = np.concatenate((fixed_features_name, selected_sorted_vector_features_name), axis=0)\n",
    "    \n",
    "    # Append fixed features importance to the list\n",
    "    sorted_features_importance.append(fixed_features_importance)\n",
    "    \n",
    "    # Append vector features importance to the list\n",
    "    for item in sorted_vector_features_importance:\n",
    "        sorted_features_importance.append(item)\n",
    "    \n",
    "    # Append fixed features importance standard deviation to the list\n",
    "    sorted_features_importance_std.append(fixed_features_importance_std)\n",
    "    \n",
    "    # Append vector features importance standard deviation to the list\n",
    "    for item in sorted_vector_features_importance_std:\n",
    "        sorted_features_importance_std.append(item)\n",
    "    \n",
    "    # Write sorted relative feature importance to file\n",
    "    font = open(result_path + \"sorted_relative_feature_importance_\" + str(screen_step) + \".dat\", 'a')\n",
    "    for i, j, k in zip(sorted_features_name, sorted_features_importance, sorted_features_importance_std):\n",
    "        print(i, j, k, file=font)\n",
    "    font.close()\n",
    "    \n",
    "    # Return the sorted features and their names\n",
    "    return sorted_features_name, selected_sorted_features\n",
    "\n",
    "\n",
    "def vector_feature_engineering(mean_features_importanceArr, std_features_importanceArr,\n",
    "                               vector_features_name, vector_features, screen_step):\n",
    "    sorted_features_importance = []\n",
    "    sorted_features_importance_std = []\n",
    "    \n",
    "    # Calculate importance of vector features\n",
    "    vector_features_importance = mean_features_importanceArr\n",
    "    vector_features_importance_std = std_features_importanceArr\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_idx = np.argsort(vector_features_importance)[::-1]\n",
    "    sorted_vector_features_name = np.array(vector_features_name)[sorted_idx]\n",
    "    sorted_vector_features_importance = np.array(vector_features_importance)[sorted_idx]\n",
    "    sorted_vector_features_importance_std = np.array(vector_features_importance_std)[sorted_idx]\n",
    "    \n",
    "    row_number = np.shape(vector_features)[0]\n",
    "    vfeature_column_number = np.shape(vector_features)[1]\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_vector_features = np.zeros((row_number, vfeature_column_number))\n",
    "    for i, j in zip(sorted_idx[1:vfeature_column_number], np.arange(0, vfeature_column_number)):\n",
    "        sorted_vector_features[:, j] = vector_features[:, i]\n",
    "    \n",
    "    # The sorted features only include vector features\n",
    "    sorted_features = sorted_vector_features\n",
    "    \n",
    "    # Remove last feature name and feature column from sorted vector features\n",
    "    selected_sorted_vector_features_name = np.delete(sorted_vector_features_name, -1)\n",
    "    selected_sorted_features = np.delete(sorted_features, -1, axis=1)\n",
    "    \n",
    "    # The sorted feature names only include vector feature names\n",
    "    sorted_features_name = selected_sorted_vector_features_name\n",
    "    \n",
    "    # Append vector features importance to the list\n",
    "    for item in sorted_vector_features_importance:\n",
    "        sorted_features_importance.append(item)\n",
    "    \n",
    "    # Append vector features importance standard deviation to the list\n",
    "    for item in sorted_vector_features_importance_std:\n",
    "        sorted_features_importance_std.append(item)\n",
    "    \n",
    "    # Write sorted relative feature importance to file\n",
    "    font = open(result_path + \"sorted_relative_feature_importance_\" + str(screen_step) + \".dat\", 'a')\n",
    "    for i, j, k in zip(sorted_features_name, sorted_features_importance, sorted_features_importance_std):\n",
    "        print(i, j, k, file=font)\n",
    "    font.close()\n",
    "    \n",
    "    # Return the sorted features and their names\n",
    "    return sorted_features_name, selected_sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a6ba5e61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Screen_step: FF, Loop: 0, No. 1, R2: 0.670423, MSE: 0.268673, MAE: 0.388422, EV: 0.674829\n",
      "Screen_step: FF, Loop: 0, No. 2, R2: 0.771627, MSE: 0.186171, MAE: 0.335074, EV: 0.773781\n",
      "Screen_step: FF, Loop: 0, No. 3, R2: 0.739993, MSE: 0.211959, MAE: 0.347296, EV: 0.740737\n",
      "Screen_step: FF, Loop: 0, No. 4, R2: 0.735277, MSE: 0.215804, MAE: 0.350325, EV: 0.736893\n",
      "Screen_step: FF, Loop: 0, No. 5, R2: 0.750011, MSE: 0.203792, MAE: 0.347852, EV: 0.752775\n",
      "Screen_step: FF, Loop: 0, No. 6, R2: 0.771849, MSE: 0.185990, MAE: 0.341329, EV: 0.773517\n",
      "Screen_step: FF, Loop: 0, No. 7, R2: 0.677374, MSE: 0.263006, MAE: 0.389301, EV: 0.677643\n",
      "Screen_step: FF, Loop: 0, No. 8, R2: 0.618923, MSE: 0.310657, MAE: 0.441021, EV: 0.722741\n",
      "Screen_step: FF, Loop: 0, No. 9, R2: 0.754966, MSE: 0.199753, MAE: 0.355267, EV: 0.756308\n",
      "Screen_step: FF, Loop: 0, No. 10, R2: 0.746195, MSE: 0.206904, MAE: 0.349745, EV: 0.749965\n",
      "Screen_step: FF, Loop: 0, No. 11, R2: -1.527421, MSE: 2.060369, MAE: 1.251476, EV: 0.267678\n",
      "Screen_step: FF, Loop: 0, No. 12, R2: 0.708112, MSE: 0.237949, MAE: 0.381394, EV: 0.708237\n",
      "Screen_step: FF, Loop: 0, No. 13, R2: 0.746433, MSE: 0.206709, MAE: 0.355311, EV: 0.746573\n",
      "Screen_step: FF, Loop: 0, No. 14, R2: -1.110135, MSE: 1.720195, MAE: 1.147926, EV: 0.461022\n",
      "Screen_step: FF, Loop: 0, No. 15, R2: 0.788800, MSE: 0.172172, MAE: 0.322266, EV: 0.789329\n",
      "Screen_step: FF, Loop: 0, No. 16, R2: 0.749224, MSE: 0.204434, MAE: 0.348319, EV: 0.749482\n",
      "Screen_step: FF, Loop: 0, No. 17, R2: 0.731800, MSE: 0.218638, MAE: 0.374063, EV: 0.737401\n",
      "Screen_step: FF, Loop: 0, No. 18, R2: 0.244107, MSE: 0.616208, MAE: 0.650878, EV: 0.484918\n",
      "Screen_step: FF, Loop: 0, No. 19, R2: 0.769525, MSE: 0.187885, MAE: 0.340867, EV: 0.784276\n",
      "Screen_step: FF, Loop: 0, No. 20, R2: 0.770114, MSE: 0.187405, MAE: 0.343892, EV: 0.772491\n",
      "Screen_step: FF, Loop: 0, No. 21, R2: 0.763228, MSE: 0.193018, MAE: 0.338934, EV: 0.766766\n",
      "Screen_step: FF, Loop: 0, No. 22, R2: 0.783131, MSE: 0.176793, MAE: 0.332282, EV: 0.784721\n",
      "Screen_step: FF, Loop: 0, No. 23, R2: 0.779109, MSE: 0.180072, MAE: 0.329751, EV: 0.780997\n",
      "Screen_step: FF, Loop: 0, No. 24, R2: 0.765678, MSE: 0.191020, MAE: 0.341873, EV: 0.767845\n",
      "Screen_step: FF, Loop: 0, No. 25, R2: 0.777539, MSE: 0.181351, MAE: 0.329987, EV: 0.779309\n",
      "Screen_step: FF, Loop: 0, No. 26, R2: 0.758804, MSE: 0.196624, MAE: 0.350688, EV: 0.758813\n",
      "Screen_step: FF, Loop: 0, No. 27, R2: 0.764777, MSE: 0.191755, MAE: 0.342410, EV: 0.765708\n",
      "Screen_step: FF, Loop: 0, No. 28, R2: 0.750657, MSE: 0.203266, MAE: 0.344334, EV: 0.754699\n",
      "Screen_step: FF, Loop: 0, No. 29, R2: 0.772081, MSE: 0.185801, MAE: 0.338474, EV: 0.773941\n",
      "Screen_step: FF, Loop: 0, No. 30, R2: 0.774772, MSE: 0.183607, MAE: 0.344425, EV: 0.776246\n",
      "Screen_step: FF, Loop: 0, No. 31, R2: 0.793864, MSE: 0.168043, MAE: 0.315894, EV: 0.794752\n",
      "Screen_step: FF, Loop: 0, No. 32, R2: 0.772345, MSE: 0.185586, MAE: 0.337395, EV: 0.772551\n",
      "Screen_step: FF, Loop: 0, No. 33, R2: 0.775769, MSE: 0.182794, MAE: 0.334776, EV: 0.777050\n",
      "Screen_step: FF, Loop: 0, No. 34, R2: 0.745601, MSE: 0.207387, MAE: 0.358292, EV: 0.748412\n",
      "Screen_step: FF, Loop: 0, No. 35, R2: 0.758902, MSE: 0.196544, MAE: 0.353769, EV: 0.759543\n",
      "Screen_step: FF, Loop: 0, No. 36, R2: 0.773040, MSE: 0.185019, MAE: 0.337541, EV: 0.774001\n",
      "Screen_step: FF, Loop: 0, No. 37, R2: 0.749354, MSE: 0.204328, MAE: 0.356810, EV: 0.751697\n",
      "Screen_step: FF, Loop: 0, No. 38, R2: 0.765989, MSE: 0.190767, MAE: 0.337440, EV: 0.769373\n",
      "Screen_step: FF, Loop: 0, No. 39, R2: 0.718703, MSE: 0.229315, MAE: 0.368038, EV: 0.721228\n",
      "Screen_step: FF, Loop: 0, No. 40, R2: 0.786419, MSE: 0.174113, MAE: 0.332319, EV: 0.787796\n",
      "Screen_step: FF, Loop: 0, No. 41, R2: 0.746840, MSE: 0.206378, MAE: 0.353269, EV: 0.749030\n",
      "Screen_step: FF, Loop: 0, No. 42, R2: 0.720686, MSE: 0.227699, MAE: 0.370965, EV: 0.723851\n",
      "Screen_step: FF, Loop: 0, No. 43, R2: 0.751866, MSE: 0.202280, MAE: 0.347359, EV: 0.753947\n",
      "Screen_step: FF, Loop: 0, No. 44, R2: 0.728795, MSE: 0.221088, MAE: 0.370944, EV: 0.731418\n",
      "Screen_step: FF, Loop: 0, No. 45, R2: 0.767205, MSE: 0.189776, MAE: 0.331744, EV: 0.767890\n",
      "Screen_step: FF, Loop: 0, No. 46, R2: 0.744050, MSE: 0.208652, MAE: 0.361561, EV: 0.745094\n",
      "Screen_step: FF, Loop: 0, No. 47, R2: 0.766644, MSE: 0.190233, MAE: 0.339231, EV: 0.767837\n",
      "Screen_step: FF, Loop: 0, No. 48, R2: 0.758716, MSE: 0.196696, MAE: 0.341749, EV: 0.760980\n",
      "Screen_step: FF, Loop: 0, No. 49, R2: 0.788978, MSE: 0.172027, MAE: 0.326986, EV: 0.790266\n",
      "Screen_step: FF, Loop: 0, No. 50, R2: 0.744826, MSE: 0.208019, MAE: 0.360384, EV: 0.745115\n",
      "Screen_step: FF, Loop: 0, No. 51, R2: 0.763298, MSE: 0.192961, MAE: 0.345779, EV: 0.764644\n",
      "Screen_step: FF, Loop: 0, No. 52, R2: 0.755407, MSE: 0.199394, MAE: 0.354650, EV: 0.756785\n",
      "Screen_step: FF, Loop: 0, No. 53, R2: 0.763714, MSE: 0.192622, MAE: 0.346169, EV: 0.765446\n",
      "Screen_step: FF, Loop: 0, No. 54, R2: 0.782272, MSE: 0.177493, MAE: 0.324529, EV: 0.784304\n",
      "Screen_step: FF, Loop: 0, No. 55, R2: 0.738456, MSE: 0.213212, MAE: 0.369787, EV: 0.739216\n",
      "Screen_step: FF, Loop: 0, No. 56, R2: 0.755283, MSE: 0.199495, MAE: 0.357810, EV: 0.757034\n",
      "Screen_step: FF, Loop: 0, No. 57, R2: 0.785019, MSE: 0.175254, MAE: 0.327957, EV: 0.785969\n",
      "Screen_step: FF, Loop: 0, No. 58, R2: 0.761262, MSE: 0.194620, MAE: 0.339468, EV: 0.761987\n",
      "Screen_step: FF, Loop: 0, No. 59, R2: 0.758155, MSE: 0.197154, MAE: 0.352671, EV: 0.760515\n",
      "Screen_step: FF, Loop: 0, No. 60, R2: 0.749244, MSE: 0.204418, MAE: 0.349431, EV: 0.759492\n",
      "Screen_step: FF, Loop: 0, No. 61, R2: 0.749241, MSE: 0.204420, MAE: 0.348169, EV: 0.749418\n",
      "Screen_step: FF, Loop: 0, No. 62, R2: 0.771486, MSE: 0.186286, MAE: 0.346744, EV: 0.772162\n",
      "Screen_step: FF, Loop: 0, No. 63, R2: 0.781725, MSE: 0.177939, MAE: 0.325644, EV: 0.782994\n",
      "Screen_step: FF, Loop: 0, No. 64, R2: 0.782861, MSE: 0.177013, MAE: 0.332750, EV: 0.783038\n",
      "Screen_step: FF, Loop: 0, No. 65, R2: 0.761605, MSE: 0.194341, MAE: 0.349439, EV: 0.762252\n",
      "Screen_step: FF, Loop: 0, No. 66, R2: 0.721773, MSE: 0.226812, MAE: 0.363649, EV: 0.723270\n",
      "Screen_step: FF, Loop: 0, No. 67, R2: 0.760652, MSE: 0.195118, MAE: 0.340269, EV: 0.761243\n",
      "Screen_step: FF, Loop: 0, No. 68, R2: 0.761405, MSE: 0.194504, MAE: 0.344715, EV: 0.762630\n",
      "Screen_step: FF, Loop: 0, No. 69, R2: 0.772830, MSE: 0.185190, MAE: 0.333000, EV: 0.773662\n",
      "Screen_step: FF, Loop: 0, No. 70, R2: 0.795916, MSE: 0.166370, MAE: 0.322422, EV: 0.796708\n",
      "Screen_step: FF, Loop: 0, No. 71, R2: 0.790813, MSE: 0.170531, MAE: 0.318483, EV: 0.791887\n",
      "Screen_step: FF, Loop: 0, No. 72, R2: 0.791074, MSE: 0.170318, MAE: 0.320492, EV: 0.792223\n",
      "Screen_step: FF, Loop: 0, No. 73, R2: 0.786434, MSE: 0.174101, MAE: 0.318249, EV: 0.788382\n",
      "Screen_step: FF, Loop: 0, No. 74, R2: 0.798316, MSE: 0.164414, MAE: 0.311831, EV: 0.799988\n",
      "Screen_step: FF, Loop: 0, No. 75, R2: 0.783286, MSE: 0.176666, MAE: 0.326523, EV: 0.785693\n",
      "Screen_step: FF, Loop: 0, No. 76, R2: 0.767351, MSE: 0.189657, MAE: 0.340991, EV: 0.780493\n",
      "Screen_step: FF, Loop: 0, No. 77, R2: 0.779148, MSE: 0.180040, MAE: 0.326847, EV: 0.782840\n",
      "Screen_step: FF, Loop: 0, No. 78, R2: 0.783555, MSE: 0.176447, MAE: 0.326920, EV: 0.786355\n",
      "Screen_step: FF, Loop: 0, No. 79, R2: 0.781225, MSE: 0.178347, MAE: 0.326647, EV: 0.783622\n",
      "Screen_step: FF, Loop: 0, No. 80, R2: 0.687662, MSE: 0.254620, MAE: 0.401555, EV: 0.756658\n",
      "Screen_step: FF, Loop: 0, No. 81, R2: 0.788896, MSE: 0.172094, MAE: 0.317523, EV: 0.791121\n",
      "Screen_step: FF, Loop: 0, No. 82, R2: 0.746443, MSE: 0.206701, MAE: 0.344220, EV: 0.749092\n",
      "Screen_step: FF, Loop: 0, No. 83, R2: 0.784118, MSE: 0.175989, MAE: 0.324008, EV: 0.784746\n",
      " 83%|████████▎ | 83/100 [00:18<00:03,  4.40trial/s, best loss: -0.7983155298955339]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3636\\324190583.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mmean_pred_trainArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_pred_trainArr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mmean_pred_testArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_pred_testArr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m     \u001b[0mmean_features_importanceArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstd_features_importanceArr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mnumber_sample\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3636\\1276369211.py\u001b[0m in \u001b[0;36mtrain_test\u001b[1;34m(trainX, testX, trainY, testY, screen_step)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mbest_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model_pred_trainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_model_pred_testY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mR2_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEV_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mR2_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMSE_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAE_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEV_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGBRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHyperParameter_Step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mpred_trainArr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model_pred_trainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mpred_testArr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbest_model_pred_testY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3636\\190067918.py\u001b[0m in \u001b[0;36mGBRegression\u001b[1;34m(trainX, testX, trainY, testY, max_evals, Loop_Step, screen_step)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mtrials\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mbest\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameter_space_gbr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_evals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'colsample_bytree'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'max_depth'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'n_estimators'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'learning_rate'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'gamma'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'min_child_weight'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[0mhyper_parameters_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoop_Step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscreen_step\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    539\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mallow_trials_fmin\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fmin\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 540\u001b[1;33m         return trials.fmin(\n\u001b[0m\u001b[0;32m    541\u001b[0m             \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    542\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfmin\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 671\u001b[1;33m         return fmin(\n\u001b[0m\u001b[0;32m    672\u001b[0m             \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m             \u001b[0mspace\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 586\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    587\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    362\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    298\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    299\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 300\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    301\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    302\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    176\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    890\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m             )\n\u001b[1;32m--> 892\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_3636\\190067918.py\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(argsDict)\u001b[0m\n\u001b[0;32m     42\u001b[0m                                \u001b[0mobjective\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"reg:squarederror\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                                )\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1023\u001b[0m                 \u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meval_metric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m             )\n\u001b[1;32m-> 1025\u001b[1;33m             self._Booster = train(\n\u001b[0m\u001b[0;32m   1026\u001b[0m                 \u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1027\u001b[0m                 \u001b[0mtrain_dmatrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcb_container\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mafter_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbst\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1916\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1917\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1918\u001b[1;33m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0m\u001b[0;32m   1919\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1920\u001b[0m                                                     dtrain.handle))\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- 运行\n",
    "# ------------------------------------------------------------------------------\n",
    "path = os.getcwd() + \"/\"\n",
    "result_file = 'Results'  # Output results folders\n",
    "result_subfile = \"/Feature_Engineering_Result_\"  # Output results subdirectory\n",
    "workbook = \"Data.xlsx\"  # Input data excel\n",
    "sheet = str(\"Sheet1\")  # Input data excel sheet\n",
    "n_fixed_features = 122  # the number of fixed features\n",
    "fixed_features_name = list(['CGMD'])  # the defined name of fixed features\n",
    "HyperParameter_Step = 100  # Hyperparameter search steps\n",
    "LoopStepMin = 0  # Minimum number of iteration loop\n",
    "LoopStepMax = 20  # Maximum  number of iteration loop\n",
    "RandomSeed = 1  # Random seed for data split\n",
    "TestSetRatio = 0.2  # Test set ratio\n",
    "number_sample = 1036  # Number of train&test data\n",
    "\n",
    "ID, formula, prototype, features, label, features_name = data_load(workbook, sheet)\n",
    "fixed_features = features[:, 0: n_fixed_features]\n",
    "initial_fixed_features_name = features_name[0:n_fixed_features]\n",
    "ScreenStep = np.shape(features[:, n_fixed_features: np.shape(features)[1]])[1]\n",
    "\n",
    "if np.shape(features)[1] > n_fixed_features:\n",
    "    if n_fixed_features != 0:\n",
    "        for i in range(0, ScreenStep):\n",
    "            screen_step = i\n",
    "            result_path = path_mkdir(path, result_subfile, screen_step)\n",
    "            mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "            mean_features_importanceArr, std_features_importanceArr, \\\n",
    "            fixed_features_name, fixed_features, vector_features_name, vector_features = feature_importance(\n",
    "                features_name,\n",
    "                features,\n",
    "                screen_step)\n",
    "            selected_sorted_features_name, selected_sorted_features = \\\n",
    "                fixed_feature_engineering(mean_features_importanceArr, std_features_importanceArr, fixed_features_name,\n",
    "                                    fixed_features, vector_features_name,\n",
    "                                    vector_features, screen_step)\n",
    "            features_name = np.concatenate(\n",
    "                (initial_fixed_features_name, selected_sorted_features_name[1: len(selected_sorted_features_name)]),\n",
    "                axis=0)\n",
    "            features = selected_sorted_features\n",
    "\n",
    "    else:\n",
    "        for i in range(0, ScreenStep):\n",
    "            screen_step = i\n",
    "            result_path = path_mkdir(path, result_subfile, screen_step)\n",
    "            mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "            mean_features_importanceArr, std_features_importanceArr, \\\n",
    "            fixed_features_name, fixed_features, vector_features_name, vector_features = feature_importance(\n",
    "                features_name,\n",
    "                features,\n",
    "                screen_step)\n",
    "            selected_sorted_features_name, selected_sorted_features = \\\n",
    "                vector_feature_engineering(mean_features_importanceArr, std_features_importanceArr,\n",
    "                                           vector_features_name, vector_features, screen_step)\n",
    "            features_name = selected_sorted_features_name\n",
    "\n",
    "            features = selected_sorted_features\n",
    "\n",
    "\n",
    "else:\n",
    "    screen_step = \"FF\"\n",
    "    result_path = path_mkdir(path, result_subfile, screen_step)\n",
    "    norm_features, norm_label = normData(features, label)\n",
    "    train_test_data_Arr, train_test_label_Arr, predict_data_Arr, predict_label_Arr = split_train_predict(\n",
    "        norm_features,\n",
    "        norm_label,\n",
    "        number_sample)\n",
    "    trainX, testX, trainY, testY = splitDataHO(train_test_data_Arr, train_test_label_Arr, TestSetRatio, RandomSeed)\n",
    "    predictX = predict_data_Arr; predictY = predict_label_Arr\n",
    "    mean_pred_trainArr, std_pred_trainArr, \\\n",
    "    mean_pred_testArr, std_pred_testArr, \\\n",
    "    mean_features_importanceArr, std_features_importanceArr = train_test(trainX, testX, trainY, testY, screen_step)\n",
    "    if number_sample != len(label):\n",
    "        predict(trainX, trainY, testX, testY, predictX, screen_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
