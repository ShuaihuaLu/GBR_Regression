{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e3e238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.environ[\"PATH\"] += os.pathsep + 'E:/Program Files (x86)/Graphviz2.38/bin'\n",
    "\n",
    "\n",
    "import xgboost\n",
    "from hyperopt import fmin, tpe, hp, rand, anneal, partial, Trials\n",
    "\n",
    "import  xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "import pandas as pd\n",
    "warnings.filterwarnings(module='sklearn*', action='ignore', category=DeprecationWarning)\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "np.set_printoptions(suppress=True)\n",
    "xlrd.xlsx.ensure_elementtree_imported(False, None)\n",
    "xlrd.xlsx.Element_has_iter = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e74229a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Build Dataset\n",
    "# ------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def data_load(filename, Sheet):\n",
    "    data = pd.read_excel(filename, sheet_name=Sheet)  # Read the specified sheet from the excel file\n",
    "    result = data.values.tolist()  # Convert the data to a nested list\n",
    "    nrows, ncols = data.shape  # Get the number of rows and columns\n",
    "\n",
    "    ID = [int(x[0]) for x in result]  # Get the ID list from the first column\n",
    "    formula = [x[1] for x in result]  # Get the chemical formula list from the second column\n",
    "    prototype = [x[2] for x in result]  # Get the initial structure name list from the third column\n",
    "    features = np.array([x[3:ncols-1] for x in result])  # Get the feature matrix from the fourth column onwards\n",
    "    label = np.array([x[-1] for x in result])  # Get the label column from the last column\n",
    "    features_name = np.array(data.columns[3:ncols-1])  # Get the feature names from the header row\n",
    "\n",
    "    return ID, formula, prototype, features, label, features_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e85530",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Handle missing values in the dataset using mean imputation\n",
    "def imputer(dataArr):\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    imp.fit(dataArr)\n",
    "    dataArr_full = imp.transform(dataArr)\n",
    "    return dataArr_full\n",
    "\n",
    "# Normalize/standardize the features in X and the target variable Y\n",
    "def normData(dataX, dataY):\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(dataX)\n",
    "    Y = scaler.fit_transform(dataY)\n",
    "    return X, Y\n",
    "\n",
    "# Split the dataset into training and testing sets using the hold-out method\n",
    "def splitDataHO(X, Y, testSize, random_state):\n",
    "    trainX, testX, trainY, testY = train_test_split(X, Y, test_size=testSize, random_state=random_state)\n",
    "    return trainX, testX, trainY, testY\n",
    "\n",
    "# Split the dataset into training/testing data and prediction data\n",
    "def split_train_predict(X, Y, number_train_test):\n",
    "    items_number = len(X)\n",
    "    train_test_data_Arr = []\n",
    "    predict_data_Arr = []\n",
    "    train_test_label_Arr = []\n",
    "    predict_label_Arr = []\n",
    "\n",
    "    for i in range(number_train_test):\n",
    "        train_test_data = X[i]\n",
    "        train_test_label = Y[i]\n",
    "        train_test_data_Arr.append(train_test_data)\n",
    "        train_test_label_Arr.append(train_test_label)\n",
    "\n",
    "    for j in range(number_train_test, items_number):\n",
    "        predict_data = X[j]\n",
    "        predict_label = Y[j]\n",
    "        predict_data_Arr.append(predict_data)\n",
    "        predict_label_Arr.append(predict_label)\n",
    "\n",
    "    return np.array(train_test_data_Arr), np.array(train_test_label_Arr), np.array(predict_data_Arr), np.array(predict_label_Arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f5fae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Model Evaluation\n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.metrics import mean_squared_error, r2_score, explained_variance_score, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.axes._axes import _log as matplotlib_axes_logger\n",
    "\n",
    "def ModelEvaluationRegression(testY, prediction):\n",
    "    # Calculate R2 score\n",
    "    R2 = r2_score(testY, prediction)\n",
    "    # Calculate mean squared error\n",
    "    MSE = mean_squared_error(testY, prediction)\n",
    "    # Calculate mean absolute error\n",
    "    MAE = mean_absolute_error(testY, prediction)\n",
    "    # Calculate explained variance\n",
    "    EV = explained_variance_score(testY, prediction)\n",
    "\n",
    "    return R2, MSE, MAE, EV\n",
    "\n",
    "def hyper_parameters_plot(parameters, trials, Loop_Step, screen_step):\n",
    "    matplotlib_axes_logger.setLevel('ERROR')\n",
    "    \n",
    "    # Create subplots for parameter visualization\n",
    "    f, axes = plt.subplots(nrows=2, ncols=3, figsize=(15, 10), dpi=330)\n",
    "    cmap = plt.cm.jet\n",
    "    \n",
    "    for i, val in enumerate(parameters):\n",
    "        # Extract parameter values and corresponding losses\n",
    "        xs = np.array([t['misc']['vals'][val] for t in trials.trials]).ravel()\n",
    "        ys = [-t['result']['loss'] for t in trials.trials]\n",
    "        xs, ys = zip(*sorted(zip(xs, ys)))\n",
    "\n",
    "        # Scatter plot of parameter values and losses\n",
    "        axes[int(i / 3), int(i % 3)].scatter(xs, ys, s=50, linewidth=0.01, alpha=0.5, norm=0.5,\n",
    "                                             c=cmap(float(i) / len(parameters)))\n",
    "        axes[int(i / 3), int(i % 3)].set_title(val, fontsize=20, fontweight=\"bold\", fontname='Arial')\n",
    "\n",
    "        # Set tick parameters and axis limits\n",
    "        axes[int(i / 3), int(i % 3)].tick_params(labelsize=18, direction='out', width=2, length=6)\n",
    "        axes[int(i / 3), int(i % 3)].set_ylim((0.5, 1))\n",
    "        labels = axes[int(i / 3), int(i % 3)].get_xticklabels() + axes[int(i / 3), int(i % 3)].get_yticklabels()\n",
    "        [label.set_fontname('Arial') for label in labels]\n",
    "        [label.set_fontweight('bold') for label in labels]\n",
    "        axes[int(i / 3), int(i % 3)].spines['top'].set_linewidth(2.5)\n",
    "        axes[int(i / 3), int(i % 3)].spines['bottom'].set_linewidth(2.5)\n",
    "        axes[int(i / 3), int(i % 3)].spines['right'].set_linewidth(2.5)\n",
    "        axes[int(i / 3), int(i % 3)].spines['left'].set_linewidth(2.5)\n",
    "    \n",
    "    # Adjust subplot layout and save the figure\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(result_path + 'hyper_parameters_trials_' + str(Loop_Step) + \"_\" + str(screen_step) + '.png', format='png')\n",
    "    plt.close()\n",
    "\n",
    "def plot_regression_results(trainY, testY, pred_trainY, std_pred_trainY, pred_testY, std_pred_testY, Loop_Step, screen_step):\n",
    "    # Calculate R2 score and mean absolute error for training set\n",
    "    R2_average_train = r2_score(trainY, pred_trainY)\n",
    "    MAE_average_train = mean_absolute_error(trainY, pred_trainY)\n",
    "    # Calculate R2 score and mean absolute error for test set\n",
    "    R2_average_test = r2_score(testY, pred_testY)\n",
    "    MAE_average_test = mean_absolute_error(testY, pred_testY)\n",
    "\n",
    "    tolerance = 0.1\n",
    "    plt.figure(figsize=(10, 10), dpi=330)\n",
    "    \n",
    "    # Plot the ideal line\n",
    "    plt.plot([testY.min()-tolerance, testY.max()+tolerance],\n",
    "             [testY.min()-tolerance, testY.max()+tolerance],\n",
    "             '--r', linewidth=3)\n",
    "    \n",
    "    # Plot the training set results with error bars\n",
    "    plt.errorbar(trainY, pred_trainY, yerr=std_pred_trainY, alpha=0.75, fmt='o', ms=13, mfc=(0 / 255, 98 / 255, 132 / 255),\n",
    "                 ecolor=(55 / 255, 60 / 255, 56 / 255), elinewidth=3, capsize=5, capthick=3,\n",
    "                 label=r'train $R^2=$ %.2f  $MAE=$ %.2f' % (R2_average_train, MAE_average_train))\n",
    "    \n",
    "    # Plot the test set results with error bars\n",
    "    plt.errorbar(testY, pred_testY, yerr=std_pred_testY, alpha=0.75, fmt='o', ms=13, mfc=(232 / 255, 48 / 255, 21 / 255),\n",
    "                 ecolor=(55 / 255, 60 / 255, 56 / 255), elinewidth=3, capsize=5, capthick=3,\n",
    "                 label=r'test $R^2=$ %.2f  $MAE=$ %.2f' % (R2_average_test, MAE_average_test))\n",
    "    \n",
    "    # Set x and y limits\n",
    "    plt.xlim([testY.min()-tolerance, testY.max()+tolerance])\n",
    "    plt.ylim([testY.min()-tolerance, testY.max()+tolerance])\n",
    "    \n",
    "    # Set x and y labels, legend, and tick parameters\n",
    "    plt.xlabel('DFT Calculated ${E_{FM-AFM}}$ (eV)', fontsize=26, fontweight=\"bold\", fontname='Arial')\n",
    "    plt.ylabel('ML Predicted ${E_{FM-AFM}}$ (eV)', fontsize=26, fontweight=\"bold\", fontname='Arial')\n",
    "    plt.legend(loc='upper left')\n",
    "    leg = plt.gca().get_legend()\n",
    "    ltext = leg.get_texts()\n",
    "    bwith = 3\n",
    "    ax = plt.gca()\n",
    "    ax.spines['bottom'].set_linewidth(bwith)\n",
    "    ax.spines['left'].set_linewidth(bwith)\n",
    "    ax.spines['top'].set_linewidth(bwith)\n",
    "    ax.spines['right'].set_linewidth(bwith)\n",
    "    plt.setp(ltext, fontsize=24, fontweight='bold', fontname='Arial')\n",
    "    plt.tick_params(direction='out', width=2, length=6)\n",
    "    labels = ax.get_xticklabels() + ax.get_yticklabels()\n",
    "    [label.set_fontname('Arial') for label in labels]\n",
    "    [label.set_fontweight('bold') for label in labels]\n",
    "    [label.set_fontsize(24) for label in labels]\n",
    "    plt.tight_layout()\n",
    "    plt.title('DFT Calculated Values vs ML Predicted Values', fontsize=26, fontweight=\"bold\", fontname='Arial')\n",
    "    \n",
    "    # Save the figure and close the plot\n",
    "    plt.savefig(result_path + \"FM_Regression_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".png\", format=\"png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8445b3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Parameter Selection\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "def GBRegression(trainX, testX, trainY, testY, max_evals, Loop_Step, screen_step):\n",
    "    # Open files for storing results\n",
    "    GBR_model_score_filename = open(result_path + \"hyper_parameter_selection_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    predicted_testY_file = open(result_path + \"predicted_testY_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".dat\", 'a')\n",
    "    predicted_trainY_file = open(result_path + \"predicted_trainY_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".dat\", 'a')\n",
    "\n",
    "    # Define the parameter space for hyperopt\n",
    "    parameter_space_gbr = {\n",
    "        \"colsample_bytree\": hp.uniform(\"colsample_bytree\", 0.5, 1),\n",
    "        \"max_depth\": hp.quniform(\"max_depth\", 1, 10, 1),\n",
    "        \"n_estimators\": hp.quniform(\"n_estimators\", 10, 200, 1),\n",
    "        \"learning_rate\": hp.uniform(\"learning_rate\", 0.01, 0.5),\n",
    "        \"subsample\": hp.uniform(\"subsample\", 0.5, 1),\n",
    "        \"min_child_weight\": hp.uniform(\"min_child_weight\", 0.5, 10),\n",
    "        \"gamma\": hp.uniform(\"gamma\", 0.01, 1)\n",
    "    }\n",
    "\n",
    "    count = 0  # Counter for parameter combinations\n",
    "\n",
    "    def function(argsDict):\n",
    "        # Extract parameter values from argsDict\n",
    "        colsample_bytree = argsDict[\"colsample_bytree\"]\n",
    "        max_depth = argsDict[\"max_depth\"]\n",
    "        n_estimators = argsDict['n_estimators']\n",
    "        learning_rate = argsDict[\"learning_rate\"]\n",
    "        subsample = argsDict[\"subsample\"]\n",
    "        min_child_weight = argsDict[\"min_child_weight\"]\n",
    "        gamma = argsDict[\"gamma\"]\n",
    "\n",
    "        # Create and train the XGBoost regression model\n",
    "        clf = xgb.XGBRegressor(nthread=4,\n",
    "                               colsample_bytree=colsample_bytree,\n",
    "                               max_depth=int(max_depth),\n",
    "                               n_estimators=int(n_estimators),\n",
    "                               learning_rate=learning_rate,\n",
    "                               subsample=subsample,\n",
    "                               min_child_weight=min_child_weight,\n",
    "                               gamma=gamma,\n",
    "                               random_state=int(42),\n",
    "                               objective=\"reg:squarederror\"\n",
    "                               )\n",
    "        clf.fit(trainX, trainY)\n",
    "        prediction = clf.predict(testX)\n",
    "\n",
    "        nonlocal count\n",
    "        count += 1\n",
    "\n",
    "        # Evaluate the model's performance\n",
    "        R2, MSE, MAE, EV = ModelEvaluationRegression(testY, prediction)\n",
    "        print(\"No.%s, R2: %f, MSE: %f, MAE: %f, EV: %f\" % (str(count), R2, MSE, MAE, EV), argsDict, file=GBR_model_score_filename)\n",
    "        print(\"Screen_step: %s, Loop: %s, No. %s, R2: %f, MSE: %f, MAE: %f, EV: %f\" %\n",
    "              (str(screen_step), str(Loop_Step), str(count), R2, MSE, MAE, EV))\n",
    "\n",
    "        # Return the negative R2 value for hyperopt to maximize\n",
    "        return -R2\n",
    "\n",
    "    trials = Trials()\n",
    "    best = fmin(function, parameter_space_gbr, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
    "    parameters = ['colsample_bytree', 'max_depth', 'n_estimators', 'learning_rate', 'gamma', 'min_child_weight']\n",
    "    hyper_parameters_plot(parameters, trials, Loop_Step, screen_step)\n",
    "\n",
    "    # Retrieve the best hyperparameters\n",
    "    colsample_bytree = best[\"colsample_bytree\"]\n",
    "    max_depth = best[\"max_depth\"]\n",
    "    n_estimators = best['n_estimators']\n",
    "    learning_rate = best[\"learning_rate\"]\n",
    "    subsample = best[\"subsample\"]\n",
    "    min_child_weight = best[\"min_child_weight\"]\n",
    "    gamma = best[\"gamma\"]\n",
    "\n",
    "    print(\"The_best_parameter：\", best, file=GBR_model_score_filename)\n",
    "\n",
    "    # Train the best model using the best hyperparameters\n",
    "    best_model = xgb.XGBRegressor(nthread=4,\n",
    "                                  colsample_bytree=colsample_bytree,\n",
    "                                  max_depth=int(max_depth),\n",
    "                                  n_estimators=int(n_estimators),\n",
    "                                  learning_rate=learning_rate,\n",
    "                                  subsample=subsample,\n",
    "                                  min_child_weight=min_child_weight,\n",
    "                                  gamma=gamma,\n",
    "                                  random_state=int(42),\n",
    "                                  objective=\"reg:squarederror\"\n",
    "                                  )\n",
    "    best_model.fit(trainX, trainY)\n",
    "    best_model_pred_testY = best_model.predict(testX)\n",
    "    best_model_pred_trainY = best_model.predict(trainX)\n",
    "    R2_test, MSE_test, MAE_test, EV_test = ModelEvaluationRegression(testY, best_model_pred_testY)\n",
    "    R2_train, MSE_train, MAE_train, EV_train = ModelEvaluationRegression(trainY, best_model_pred_trainY)\n",
    "    print(\"The_best_model_train_score:\", R2_train, MSE_train, MAE_train, EV_train, file=GBR_model_score_filename)\n",
    "    print(\"The_best_model_test_score:\", R2_test, MSE_test, MAE_test, EV_test, file=GBR_model_score_filename)\n",
    "\n",
    "    # Save the best model and visualization\n",
    "    best_model.save_model(result_path + \"best_model_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".model\")\n",
    "    digraph = xgboost.to_graphviz(best_model)\n",
    "    digraph_name = result_path + \"plot_best_tree_\" + str(Loop_Step) + \"_\" + str(screen_step) + \".gv\"\n",
    "    digraph.save(filename=digraph_name)\n",
    "\n",
    "    # Write predicted values to files\n",
    "    for y in best_model_pred_testY:\n",
    "        print(y, file=predicted_testY_file)\n",
    "    for y in best_model_pred_trainY:\n",
    "        print(y, file=predicted_trainY_file)\n",
    "\n",
    "    # Close the files\n",
    "    GBR_model_score_filename.close()\n",
    "    predicted_testY_file.close()\n",
    "    predicted_trainY_file.close()\n",
    "\n",
    "    return best_model, best_model_pred_trainY, best_model_pred_testY, \\\n",
    "           R2_train, MSE_train, MAE_train, EV_train, \\\n",
    "           R2_test, MSE_test, MAE_test, EV_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea128c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- Prediction\n",
    "# ------------------------------------------------------------------------------\n",
    "\n",
    "# Function to create directory if it doesn't exist\n",
    "def path_mkdir(path, result_subfile, screen_step):\n",
    "    feature_engineering_result_file = result_file + str(result_subfile) + str(screen_step)\n",
    "    if not os.path.exists(result_file):\n",
    "        os.mkdir(path + result_file)\n",
    "    if not os.path.exists(feature_engineering_result_file):\n",
    "        os.mkdir(path + feature_engineering_result_file)\n",
    "    result_path = path + feature_engineering_result_file + \"/\"\n",
    "    return result_path\n",
    "\n",
    "def train_test(trainX, testX, trainY, testY, screen_step):\n",
    "    pred_testArr = []              # List to store predicted test values\n",
    "    mean_pred_testArr = []         # List to store mean of predicted test values\n",
    "    std_pred_testArr = []          # List to store standard deviation of predicted test values\n",
    "    pred_trainArr = []             # List to store predicted train values\n",
    "    mean_pred_trainArr = []        # List to store mean of predicted train values\n",
    "    std_pred_trainArr = []         # List to store standard deviation of predicted train values\n",
    "    features_importanceArr = []    # List to store feature importances\n",
    "    mean_features_importanceArr = []   # List to store mean of feature importances\n",
    "    std_features_importanceArr = []    # List to store standard deviation of feature importances\n",
    "\n",
    "    for n in range(LoopStepMin, LoopStepMax):\n",
    "        # Perform GBRegression and store the results\n",
    "        best_model, best_model_pred_trainY, best_model_pred_testY, R2_train, MSE_train, MAE_train, EV_train, R2_test, MSE_test, MAE_test, EV_test = \\\n",
    "            GBRegression(trainX, testX, trainY, testY, HyperParameter_Step, n, screen_step)\n",
    "        pred_trainArr.append(best_model_pred_trainY)\n",
    "        pred_testArr.append(best_model_pred_testY)\n",
    "        features_importance = best_model.feature_importances_\n",
    "        features_importanceArr.append(features_importance)\n",
    "    \n",
    "    # Calculate mean and standard deviation of predicted test values\n",
    "    for i in range(np.shape(pred_testArr)[1]):\n",
    "        sample = np.array(pred_testArr)[:, i]\n",
    "        testsample_prediction_mean = np.mean(sample)\n",
    "        testsample_prediction_std = np.std(sample, ddof=0)\n",
    "        mean_pred_testArr.append(testsample_prediction_mean)\n",
    "        std_pred_testArr.append(testsample_prediction_std)\n",
    "    \n",
    "    # Calculate mean and standard deviation of predicted train values\n",
    "    for j in range(np.shape(pred_trainArr)[1]):\n",
    "        sample = np.array(pred_trainArr)[:, j]\n",
    "        trainsample_prediction_mean = np.mean(sample)\n",
    "        trainsample_prediction_std = np.std(sample, ddof=0)\n",
    "        mean_pred_trainArr.append(trainsample_prediction_mean)\n",
    "        std_pred_trainArr.append(trainsample_prediction_std)\n",
    "    \n",
    "    # Calculate mean and standard deviation of feature importances\n",
    "    for k in range(np.shape(features_importanceArr)[1]):\n",
    "        sample = np.array(features_importanceArr)[:, k]\n",
    "        features_importance_mean = np.mean(sample)\n",
    "        features_importance_std = np.std(sample, ddof=0)\n",
    "        mean_features_importanceArr.append(features_importance_mean)\n",
    "        std_features_importanceArr.append(features_importance_std)\n",
    "\n",
    "    # Plot regression results\n",
    "    plot_regression_results(trainY, testY, np.array(mean_pred_trainArr), np.array(std_pred_trainArr), np.array(mean_pred_testArr), \\\n",
    "                            np.array(std_pred_testArr), 'average', screen_step)\n",
    "\n",
    "    # Open files for writing predicted train, test, and feature importance values\n",
    "    train_font = open(result_path + \"predicted_train_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    test_font = open(result_path + \"predicted_test_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    features_importance_font = open(result_path + \"features_importance_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    \n",
    "    # Write mean and standard deviation of predicted train values to file\n",
    "    for i, j in zip(mean_pred_trainArr, std_pred_trainArr):\n",
    "        print(i, j, file=train_font)\n",
    "    \n",
    "    # Write mean and standard deviation of predicted test values to file\n",
    "    for i, j in zip(mean_pred_testArr, std_pred_testArr):\n",
    "        print(i, j, file=test_font)\n",
    "    \n",
    "    # Write feature names, mean, and standard deviation of feature importances to file\n",
    "    for i, j, k in zip(features_name, mean_features_importanceArr, std_features_importanceArr):\n",
    "        print(i, j, k, file=features_importance_font)\n",
    "    \n",
    "    # Close the files\n",
    "    train_font.close()\n",
    "    test_font.close()\n",
    "    features_importance_font.close()\n",
    "    \n",
    "    # Return the calculated values\n",
    "    return mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, mean_features_importanceArr, std_features_importanceArr\n",
    "\n",
    "def predict(trainX, trainY, testX, testY, predictX, screen_step):\n",
    "    print(\"Predicting......\")\n",
    "    predict_font = open(result_path + \"predicted_predict_average_\" + str(screen_step) + \".dat\", \"a\")\n",
    "    train_set = pd.DataFrame(trainX, columns=features_name)\n",
    "    dtrain = xgboost.DMatrix(train_set)\n",
    "    test_set = pd.DataFrame(testX, columns=features_name)\n",
    "    dtest = xgboost.DMatrix(test_set)\n",
    "    predict_set = pd.DataFrame(predictX, columns=features_name)\n",
    "    dpredict = xgboost.DMatrix(predict_set)\n",
    "    pred_predict_Arr = []               # List to store predicted values for predictX\n",
    "    mean_pred_predict_Arr = []          # List to store mean of predicted values for predictX\n",
    "    std_pred_predict_Arr = []           # List to store standard deviation of predicted values for predictX\n",
    "    \n",
    "    for i in range(LoopStepMin, LoopStepMax):\n",
    "        model_file = result_path + \"best_model_\" + str(i) + \"_FF.model\"\n",
    "        best_model = xgboost.Booster(model_file=model_file)\n",
    "        best_model_pred_testY = best_model.predict(dtest)\n",
    "        best_model_pred_trainY = best_model.predict(dtrain)\n",
    "        best_model_pred_predictY = best_model.predict(dpredict)\n",
    "        R2_test, MSE_test, MAE_test, EV_test = ModelEvaluationRegression(testY, best_model_pred_testY)\n",
    "        R2_train, MSE_train, MAE_train, EV_train = ModelEvaluationRegression(trainY, best_model_pred_trainY)\n",
    "        print(\"R2_train: %f, MSE_train: %f, MAE_train: %f, EV_train: %f\" % (R2_train, MSE_train, MAE_train, EV_train))\n",
    "        print(\"R2_test: %f, MSE_test: %f, MAE_test: %f, EV_test: %f\" % (R2_test, MSE_test, MAE_test, EV_test))\n",
    "        pred_predict_Arr.append(best_model_pred_predictY)\n",
    "    \n",
    "    # Calculate mean and standard deviation of predicted values for predictX\n",
    "    for i in range(np.shape(pred_predict_Arr)[1]):\n",
    "        sample = np.array(pred_predict_Arr)[:, i]\n",
    "        predictsample_prediction_mean = np.mean(sample)\n",
    "        predictsample_prediction_std = np.std(sample, ddof=0)\n",
    "        mean_pred_predict_Arr.append(predictsample_prediction_mean)\n",
    "        std_pred_predict_Arr.append(predictsample_prediction_std)\n",
    "    \n",
    "    # Write mean and standard deviation of predicted values for predictX to file\n",
    "    for i, j in zip(mean_pred_predict_Arr, std_pred_predict_Arr):\n",
    "        print(i, j, file=predict_font)\n",
    "    \n",
    "    # Return the calculated values\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d56dc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# --- feature engineering\n",
    "# -----------------------------------------------------------------------------\n",
    "def feature_importance(features_name, features, screen_step):\n",
    "    # Open files to write normalized and original dataset\n",
    "    normdata_filename = open(result_path + \"norm_dataset_\" + str(screen_step) +\".dat\", \"a\")\n",
    "    data_filename = open(result_path + \"dataset_\" + str(screen_step) +\".dat\", \"a\")\n",
    "    \n",
    "    if n_fixed_features != 0:\n",
    "        # Extract vector features\n",
    "        vector_features = features[:, n_fixed_features: np.shape(features)[1]]\n",
    "        vector_features_name = list(features_name[n_fixed_features: np.shape(features)[1]])\n",
    "        \n",
    "        # Concatenate fixed features with vector features\n",
    "        new_features = np.concatenate((fixed_features, vector_features), axis=1)\n",
    "        \n",
    "        # Normalize features and labels\n",
    "        norm_features, norm_label = normData(new_features, label)\n",
    "        \n",
    "        # Concatenate fixed feature names with vector feature names\n",
    "        new_features_name = np.concatenate((initial_fixed_features_name, vector_features_name), axis=0)\n",
    "        \n",
    "        # Write normalized dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in new_features_name], \"label\", file=normdata_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, norm_features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=normdata_filename)\n",
    "        \n",
    "        # Write original dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in new_features_name], \"label\", file=data_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=data_filename)\n",
    "    else:\n",
    "        # Use all features as vector features\n",
    "        vector_features = features\n",
    "        vector_features_name = list(features_name)\n",
    "        \n",
    "        # Normalize vector features and labels\n",
    "        norm_vector_features, norm_label = normData(vector_features, label)\n",
    "        norm_features = norm_vector_features\n",
    "        \n",
    "        # Write normalized dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in vector_features_name], \"label\", file=normdata_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, norm_features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=normdata_filename)\n",
    "        \n",
    "        # Write original dataset to file\n",
    "        print(\"ID, formula, prototype\", [name for name in vector_features_name], \"label\", file=data_filename)\n",
    "        for i, j, k, x, y in zip(ID, formula, prototype, vector_features, label):\n",
    "            print(i, j, k, [item for item in x], y, file=data_filename)\n",
    "\n",
    "    # Split data into train-test sets\n",
    "    train_test_data_Arr, train_test_label_Arr, predict_data_Arr, predict_label_Arr = split_train_predict(norm_features,\n",
    "                                                                                                         norm_label,\n",
    "                                                                                                         number_sample)\n",
    "    trainX, testX, trainY, testY = splitDataHO(train_test_data_Arr, train_test_label_Arr, TestSetRatio, RandomSeed)\n",
    "\n",
    "    # Train and test the model\n",
    "    mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "    mean_features_importanceArr, std_features_importanceArr = train_test(trainX, testX, trainY, testY, screen_step)\n",
    "\n",
    "    normdata_filename.close()\n",
    "    data_filename.close()\n",
    "    \n",
    "    # Return the results\n",
    "    return mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "           mean_features_importanceArr, std_features_importanceArr, \\\n",
    "           fixed_features_name, fixed_features, vector_features_name, vector_features\n",
    "\n",
    "\n",
    "def fixed_feature_engineering(mean_features_importanceArr, std_features_importanceArr, fixed_features_name,\n",
    "                              fixed_features, vector_features_name, vector_features, screen_step):\n",
    "    sorted_features_importance = []\n",
    "    sorted_features_importance_std = []\n",
    "    \n",
    "    shape_fixed_features = np.shape(fixed_features)\n",
    "    \n",
    "    # Calculate importance of fixed features\n",
    "    fixed_features_importance = np.float(np.sum(mean_features_importanceArr[0: shape_fixed_features[1]]))\n",
    "    fixed_features_importance_std = np.sum(std_features_importanceArr[0: shape_fixed_features[1]]) / shape_fixed_features[1]\n",
    "    \n",
    "    # Calculate importance of vector features\n",
    "    vector_features_importance = mean_features_importanceArr[shape_fixed_features[1]: len(mean_features_importanceArr)]\n",
    "    vector_features_importance_std = std_features_importanceArr[shape_fixed_features[1]: len(mean_features_importanceArr)]\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_idx = np.argsort(vector_features_importance)[::-1]\n",
    "    sorted_vector_features_name = np.array(vector_features_name)[sorted_idx]\n",
    "    sorted_vector_features_importance = np.array(vector_features_importance)[sorted_idx]\n",
    "    sorted_vector_features_importance_std = np.array(vector_features_importance_std)[sorted_idx]\n",
    "    \n",
    "    row_number = np.shape(vector_features)[0]\n",
    "    vfeature_column_number = np.shape(vector_features)[1]\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_vector_features = np.zeros((row_number, vfeature_column_number))\n",
    "    for i, j in zip(sorted_idx, np.arange(0, vfeature_column_number)):\n",
    "        sorted_vector_features[:, j] = vector_features[:, i]\n",
    "    \n",
    "    # Concatenate fixed features with sorted vector features\n",
    "    sorted_features = np.concatenate((fixed_features, sorted_vector_features), axis=1)\n",
    "    \n",
    "    # Remove last feature name and feature column from sorted vector features\n",
    "    selected_sorted_vector_features_name = np.delete(sorted_vector_features_name, -1)\n",
    "    selected_sorted_features = np.delete(sorted_features, -1, axis=1)\n",
    "    \n",
    "    # Concatenate fixed feature names with sorted vector feature names\n",
    "    sorted_features_name = np.concatenate((fixed_features_name, selected_sorted_vector_features_name), axis=0)\n",
    "    \n",
    "    # Append fixed features importance to the list\n",
    "    sorted_features_importance.append(fixed_features_importance)\n",
    "    \n",
    "    # Append vector features importance to the list\n",
    "    for item in sorted_vector_features_importance:\n",
    "        sorted_features_importance.append(item)\n",
    "    \n",
    "    # Append fixed features importance standard deviation to the list\n",
    "    sorted_features_importance_std.append(fixed_features_importance_std)\n",
    "    \n",
    "    # Append vector features importance standard deviation to the list\n",
    "    for item in sorted_vector_features_importance_std:\n",
    "        sorted_features_importance_std.append(item)\n",
    "    \n",
    "    # Write sorted relative feature importance to file\n",
    "    font = open(result_path + \"sorted_relative_feature_importance_\" + str(screen_step) + \".dat\", 'a')\n",
    "    for i, j, k in zip(sorted_features_name, sorted_features_importance, sorted_features_importance_std):\n",
    "        print(i, j, k, file=font)\n",
    "    font.close()\n",
    "    \n",
    "    # Return the sorted features and their names\n",
    "    return sorted_features_name, selected_sorted_features\n",
    "\n",
    "\n",
    "def vector_feature_engineering(mean_features_importanceArr, std_features_importanceArr,\n",
    "                               vector_features_name, vector_features, screen_step):\n",
    "    sorted_features_importance = []\n",
    "    sorted_features_importance_std = []\n",
    "    \n",
    "    # Calculate importance of vector features\n",
    "    vector_features_importance = mean_features_importanceArr\n",
    "    vector_features_importance_std = std_features_importanceArr\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_idx = np.argsort(vector_features_importance)[::-1]\n",
    "    sorted_vector_features_name = np.array(vector_features_name)[sorted_idx]\n",
    "    sorted_vector_features_importance = np.array(vector_features_importance)[sorted_idx]\n",
    "    sorted_vector_features_importance_std = np.array(vector_features_importance_std)[sorted_idx]\n",
    "    \n",
    "    row_number = np.shape(vector_features)[0]\n",
    "    vfeature_column_number = np.shape(vector_features)[1]\n",
    "    \n",
    "    # Sort vector features based on importance\n",
    "    sorted_vector_features = np.zeros((row_number, vfeature_column_number))\n",
    "    for i, j in zip(sorted_idx[1:vfeature_column_number], np.arange(0, vfeature_column_number)):\n",
    "        sorted_vector_features[:, j] = vector_features[:, i]\n",
    "    \n",
    "    # The sorted features only include vector features\n",
    "    sorted_features = sorted_vector_features\n",
    "    \n",
    "    # Remove last feature name and feature column from sorted vector features\n",
    "    selected_sorted_vector_features_name = np.delete(sorted_vector_features_name, -1)\n",
    "    selected_sorted_features = np.delete(sorted_features, -1, axis=1)\n",
    "    \n",
    "    # The sorted feature names only include vector feature names\n",
    "    sorted_features_name = selected_sorted_vector_features_name\n",
    "    \n",
    "    # Append vector features importance to the list\n",
    "    for item in sorted_vector_features_importance:\n",
    "        sorted_features_importance.append(item)\n",
    "    \n",
    "    # Append vector features importance standard deviation to the list\n",
    "    for item in sorted_vector_features_importance_std:\n",
    "        sorted_features_importance_std.append(item)\n",
    "    \n",
    "    # Write sorted relative feature importance to file\n",
    "    font = open(result_path + \"sorted_relative_feature_importance_\" + str(screen_step) + \".dat\", 'a')\n",
    "    for i, j, k in zip(sorted_features_name, sorted_features_importance, sorted_features_importance_std):\n",
    "        print(i, j, k, file=font)\n",
    "    font.close()\n",
    "    \n",
    "    # Return the sorted features and their names\n",
    "    return sorted_features_name, selected_sorted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6ba5e61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12192\\2219114409.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# --- 运行\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# ------------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mresult_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'Results'\u001b[0m  \u001b[1;31m# Output results folders\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mresult_subfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"/Feature_Engineering_Result_\"\u001b[0m  \u001b[1;31m# Output results subdirectory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------------------\n",
    "# --- 运行\n",
    "# ------------------------------------------------------------------------------\n",
    "path = os.getcwd() + \"/\"\n",
    "result_file = 'Results'  # Output results folders\n",
    "result_subfile = \"/Feature_Engineering_Result_\"  # Output results subdirectory\n",
    "workbook = \"Data.xlsx\"  # Input data excel\n",
    "sheet = str(\"Sheet1\")  # Input data excel sheet\n",
    "n_fixed_features = 122  # the number of fixed features\n",
    "fixed_features_name = list(['CGMD'])  # the defined name of fixed features\n",
    "HyperParameter_Step = 100  # Hyperparameter search steps\n",
    "LoopStepMin = 0  # Minimum number of iteration loop\n",
    "LoopStepMax = 20  # Maximum  number of iteration loop\n",
    "RandomSeed = 1  # Random seed for data split\n",
    "TestSetRatio = 0.2  # Test set ratio\n",
    "number_sample = 1036  # Number of train&test data\n",
    "\n",
    "ID, formula, prototype, features, label, features_name = data_load(workbook, sheet)\n",
    "fixed_features = features[:, 0: n_fixed_features]\n",
    "initial_fixed_features_name = features_name[0:n_fixed_features]\n",
    "ScreenStep = np.shape(features[:, n_fixed_features: np.shape(features)[1]])[1]\n",
    "\n",
    "if np.shape(features)[1] > n_fixed_features:\n",
    "    if n_fixed_features != 0:\n",
    "        for i in range(0, ScreenStep):\n",
    "            screen_step = i\n",
    "            result_path = path_mkdir(path, result_subfile, screen_step)\n",
    "            mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "            mean_features_importanceArr, std_features_importanceArr, \\\n",
    "            fixed_features_name, fixed_features, vector_features_name, vector_features = feature_importance(\n",
    "                features_name,\n",
    "                features,\n",
    "                screen_step)\n",
    "            selected_sorted_features_name, selected_sorted_features = \\\n",
    "                fixed_feature_engineering(mean_features_importanceArr, std_features_importanceArr, fixed_features_name,\n",
    "                                    fixed_features, vector_features_name,\n",
    "                                    vector_features, screen_step)\n",
    "            features_name = np.concatenate(\n",
    "                (initial_fixed_features_name, selected_sorted_features_name[1: len(selected_sorted_features_name)]),\n",
    "                axis=0)\n",
    "            features = selected_sorted_features\n",
    "\n",
    "    else:\n",
    "        for i in range(0, ScreenStep):\n",
    "            screen_step = i\n",
    "            result_path = path_mkdir(path, result_subfile, screen_step)\n",
    "            mean_pred_trainArr, std_pred_trainArr, mean_pred_testArr, std_pred_testArr, \\\n",
    "            mean_features_importanceArr, std_features_importanceArr, \\\n",
    "            fixed_features_name, fixed_features, vector_features_name, vector_features = feature_importance(\n",
    "                features_name,\n",
    "                features,\n",
    "                screen_step)\n",
    "            selected_sorted_features_name, selected_sorted_features = \\\n",
    "                vector_feature_engineering(mean_features_importanceArr, std_features_importanceArr,\n",
    "                                           vector_features_name, vector_features, screen_step)\n",
    "            features_name = selected_sorted_features_name\n",
    "\n",
    "            features = selected_sorted_features\n",
    "\n",
    "\n",
    "else:\n",
    "    screen_step = \"FF\"\n",
    "    result_path = path_mkdir(path, result_subfile, screen_step)\n",
    "    norm_features, norm_label = normData(features, label)\n",
    "    train_test_data_Arr, train_test_label_Arr, predict_data_Arr, predict_label_Arr = split_train_predict(\n",
    "        norm_features,\n",
    "        norm_label,\n",
    "        number_sample)\n",
    "    trainX, testX, trainY, testY = splitDataHO(train_test_data_Arr, train_test_label_Arr, TestSetRatio, RandomSeed)\n",
    "    predictX = predict_data_Arr; predictY = predict_label_Arr\n",
    "    mean_pred_trainArr, std_pred_trainArr, \\\n",
    "    mean_pred_testArr, std_pred_testArr, \\\n",
    "    mean_features_importanceArr, std_features_importanceArr = train_test(trainX, testX, trainY, testY, screen_step)\n",
    "    if number_sample != len(label):\n",
    "        predict(trainX, trainY, testX, testY, predictX, screen_step)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
